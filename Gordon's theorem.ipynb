{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction using Gordon's theorem\n",
    "\n",
    "The Johnson-Lindenstrauss lemma guarantees that with high probability a random projection $\\Phi$ will preserve the inter-point distances and angles in a data set $X\\subset \\mathbb{R}^d$ (of $n$ samples) up to a distortion parameter $\\varepsilon$ in the sense that\n",
    "\n",
    "$$\n",
    "  (1 - \\varepsilon) \\| x - y \\|^2 < \\| \\Phi(x) - \\Phi(y) \\|^2 < (1 + \\varepsilon) \\| x - y \\|^2, \\quad \\forall x,y\\in X\n",
    "$$\n",
    "\n",
    "as long as target dimension $m$ satisfies\n",
    "\n",
    "$$\n",
    "  m \\geq \\frac{4\\log n}{\\varepsilon^2 / 2 - \\varepsilon^3 / 3} .\n",
    "$$\n",
    "\n",
    "Note that this lower bound is a worst case scenario estimate as it doesn't depend on $X$. This has been implemented in skicit-learn module `random_projection.py` as function `johnson_lindenstrauss_min_dim`.\n",
    "    \n",
    "Gordon's theorem [1] improves the lower bound on target dimension by taking into account the geometric complexity of $X$. More precisely, there is a universal constant $c > 0$ for which the lower bound\n",
    "\n",
    "$$\n",
    "  m \\geq c \\, \\frac{g(T)^2 + 1}{\\varepsilon^2}\n",
    "$$\n",
    "\n",
    "guarantees that, on average, the distortion doesn't exceed $\\varepsilon$ in the sense that\n",
    "\n",
    "$$\n",
    "  \\mathbb{E} \\max_{x \\in T} \\big| \\, \\| \\Phi(x) \\|^2 - 1 \\, \\big| < \\varepsilon .\n",
    "$$\n",
    "\n",
    "Here the geometric complexity of $X$ is measured by\n",
    "\n",
    "$$\n",
    "  g(T) = \\mathbb{E} \\max_{x \\in T} \\big| \\, \\langle \\gamma , x \\rangle \\, \\big| ,\n",
    "$$\n",
    "\n",
    "where $T$ is the set of normalized difference vectors of $X$ and $\\gamma$ is a standard $d$-dimensional normal variable.\n",
    "\n",
    "This presentation follows [2], which contains numerous ideas for further improvements.\n",
    "\n",
    "References:\n",
    "\n",
    "[1] Y. Gordon: On Milman's inequality and random subspaces which escape through a mesh in R^n.\n",
    "    _Geometric Aspects of Functional Analysis_, 84--106, 1988.\n",
    "\n",
    "[2] J. Bourgain, S. Dirksen, J. Nelson: Toward a unified theory of sparse dimensionality reduction in Euclidean space.\n",
    "    _Geometric and Functional Analysis_, 25(4): 1009--1088, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a function to compute normalized difference vectors\n",
    "def diffvect(X):\n",
    "    \"\"\"\n",
    "    Input: Data matrix X as a numpy array of shape [n_samples, n_features]\n",
    "    Returns: Numpy array T of normalized difference vectors with shape [n_diffs, n_features], where\n",
    "    n_diffs = n_samples * (n_samples - 1) / 2\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    meps = np.finfo(float).eps\n",
    "    T = np.array([(X[i] - X[j]) / max(np.linalg.norm(X[i]-X[j]), meps)\n",
    "         for i in range(n_samples) for j in range(i)])\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the geometric complexity and the target dimension according to Gordon's theorem\n",
    "def gordon_min_dim(X, eps=0.1):\n",
    "    \"\"\"\n",
    "    Input: Data matrix X as a numpy array of shape [n_samples, n_features]\n",
    "    Returns: A suggestion for the target dimension m\n",
    "    Note: This implementation is still quite slow.\n",
    "          Also, the universal constant c should be determined\n",
    "          more carefully.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    T = diffvect(X)\n",
    "    N = 1000  # number of samples from the normal distribution, increase for more reliability\n",
    "    G = np.random.randn(N, n_features)\n",
    "    # G.shape = (N, n_features)\n",
    "    # T.shape = (n_diffs, n_features)\n",
    "    # np.dot(G,T.T).shape = (N, n_diffs)\n",
    "    # np.amax(np.abs(_), axis=1).shape = (N,)\n",
    "    g_T = np.mean(np.amax(np.abs(np.dot(G,T.T)), axis=1))\n",
    "    c = 0.7  # experimentally adjusted\n",
    "    m = c * ((g_T**2 + 1)/eps**2).astype(np.int)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import random_projection\n",
    "\n",
    "# Define a function to measure the distortion induced on the dataset by a random projection\n",
    "def test(X, m):\n",
    "    \"\"\"Measure the distortion on X from random projection with target dimension m.\n",
    "    Note that Gordon's theorem states that the expectation of the distortion doesn't\n",
    "    exceed eps, whereas here we consider individual random projections.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    Phi = random_projection.gaussian_random_matrix(m, n_features)\n",
    "    T = diffvect(X)\n",
    "    # T.shape = (n_diffs, n_features)\n",
    "    # Phi.shape = (m, n_features)\n",
    "    # np.dot(T, Phi.T).shape = (n_diffs, m)\n",
    "    # np.linalg.norm(_, axis=1).shape = (n_diffs,)\n",
    "    norms = np.linalg.norm(np.dot(T, Phi.T), axis=1)\n",
    "    # np.abs(norms - 1).shape = (n_diffs,)\n",
    "    return np.max(np.abs(norms - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then compare Johnson-Lindenstrauss lemma with Gordon's theorem in dimensionality reduction of two datasets of $n$ samples, namely the Olivetti faces dataset and a random dataset of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "n = 100  # number of samples\n",
    "eps = 0.1  # allowed amount of distortion\n",
    "dataset = fetch_olivetti_faces()\n",
    "X_faces = dataset.data[:n,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 samples of 4096-dimensional face image data.\n",
      "\n",
      "Johnson-Lindenstrauss lemma\n",
      "Target dimension: 3947\n",
      "Distortion for sample projection: 0.041 < 0.1\n",
      "\n",
      "Gordon's theorem\n",
      "Target dimension: 811\n",
      "Distortion for sample projection: 0.099 < 0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"{} samples of 4096-dimensional face image data.\".format(n))\n",
    "\n",
    "m_JL_n = random_projection.johnson_lindenstrauss_min_dim(n, eps=eps)\n",
    "m_G_faces = gordon_min_dim(X_faces)\n",
    "\n",
    "print()\n",
    "print(\"Johnson-Lindenstrauss lemma\")\n",
    "print(\"Target dimension: {:.0f}\".format(m_JL_n))\n",
    "if m_JL_n < 4096:\n",
    "    d_JL_faces = test(X_faces, int(m_JL_n))\n",
    "    comparison = \" < \" if d_JL_faces < eps else \" > \"\n",
    "    print(\"Distortion for sample projection: {:.3f}\".format(d_JL_faces) + comparison + str(eps))\n",
    "else:\n",
    "    print(\"Target dimension too large.\")\n",
    "\n",
    "print()\n",
    "print(\"Gordon's theorem\")\n",
    "print(\"Target dimension: {:.0f}\".format(m_G_faces))\n",
    "if m_G_faces < 4096:\n",
    "    d_G_faces = test(X_faces, int(m_G_faces))\n",
    "    comparison = \" < \" if d_G_faces < eps else \" > \"\n",
    "    print(\"Distortion for sample projection: {:.3f}\".format(d_G_faces) + comparison + str(eps))\n",
    "else:\n",
    "    print(\"Target dimension too large.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 random samples from 4096-dimensional normal distribution.\n",
      "\n",
      "Johnson-Lindenstrauss lemma\n",
      "Target dimension: 3947\n",
      "Distortion for sample projection: 0.042 < 0.1\n",
      "\n",
      "Gordon's theorem\n",
      "Target dimension: 956\n",
      "Distortion for sample projection: 0.098 < 0.1\n"
     ]
    }
   ],
   "source": [
    "d = 4096\n",
    "\n",
    "print(\"{} random samples from {}-dimensional normal distribution.\".format(n,d))\n",
    "\n",
    "X_random = np.random.randn(n,d)\n",
    "m_G_random = gordon_min_dim(X_random)\n",
    "\n",
    "print()\n",
    "print(\"Johnson-Lindenstrauss lemma\")\n",
    "print(\"Target dimension: {:.0f}\".format(m_JL_n))\n",
    "if m_JL_n < d:\n",
    "    d_JL_random = test(X_random, int(m_JL_n))\n",
    "    comparison = \" < \" if d_JL_random < eps else \" > \"\n",
    "    print(\"Distortion for sample projection: {:.3f}\".format(d_JL_random) + comparison + str(eps))\n",
    "else:\n",
    "    print(\"Target dimension too large.\")\n",
    "\n",
    "print()\n",
    "print(\"Gordon's theorem\")\n",
    "print(\"Target dimension: {:.0f}\".format(m_G_random))\n",
    "if m_G_random < d:\n",
    "    d_G_random = test(X_random, int(m_G_random))\n",
    "    comparison = \" < \" if d_G_random < eps else \" > \"\n",
    "    print(\"Distortion for sample projection: {:.3f}\".format(d_G_random) + comparison + str(eps))\n",
    "else:\n",
    "    print(\"Target dimension too large.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dimension 811 for faces is smaller than the target dimension 956 for random data. In other words, the geometric complexity of the face data is smaller than that of random data.\n"
     ]
    }
   ],
   "source": [
    "if m_G_faces < m_G_random:\n",
    "    print(\"Target dimension {:.0f} for faces is smaller than the target dimension {:.0f} for random data. In other words, the geometric complexity of the face data is smaller than that of random data.\".format(m_G_faces, m_G_random))\n",
    "else:\n",
    "    print(\"Surprisingly, the target dimension {:.0f} for faces is greater than the target dimension {:.0f} for random data. In other words, the geometric complexity of the face data is greater than that of random data.\".format(m_G_faces, m_G_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
